---
layout: post
title: "Assumptions of Statistics Analysis"
author: "Gaoping Huang"
---

A brief summary of the assumptions of statistic methods from the book "Discovering Statistics using R" by Andy Field.

To test these assumptions with R, see [Summary of Statistics Methods using R](/2017/11/01/statistics-methods-using-R).

## Parametric data
### 1. Normally distributed data
It means different things in different contexts.

### 2. Homogeneity of variance
This assumption means that the variances should be the same throughout the data.

### 3. Interval data
Data should be measured at least at the interval level.

### 4. Independence
This assumption, like that of normality, is different depending on the test you’re using.



## ANCOVA

### 1. Independence of the covariate and treatment effect
For example, anxiety and depression are closely correlated (anxious people tend to be depressed) so if you wanted to compare an anxious group of people against a non-anxious group on some task, the chances are that the anxious group would also be more depressed than the non-anxious group. You might think that by adding depression as a covariate into the analysis you can look at the ‘pure’ effect of anxiety, but you can’t.

### 2. Homogeneity of regression slopes
For example, if there’s a positive relationship between the covariate and the outcome in one group, we assume that there is a positive relationship in all of the other groups too.

If you have violated the assumption of homogeneity of regression slopes, or if the variability in regression slopes is an interesting hypothesis in itself, then you can explicitly model this variation using multilevel linear models (see Chapter 19).

